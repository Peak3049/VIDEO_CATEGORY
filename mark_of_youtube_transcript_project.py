# -*- coding: utf-8 -*-
"""Mark of Youtube Transcript Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ea-IM6a7r5M2_NMxCZ_GAZlqSqEIFc3D

## Step 1: CSV Files (Upload the database)
"""

!pip install yt-dlp

from google.colab import drive
drive.mount('/content/drive')

import os
import zipfile

# Path to the zip file in Google Drive
# Please update this path with the correct location of your archive.zip file
zip_file_path = '/content/drive/My Drive/youtube_analysis/archive.zip'

# Destination directory to extract the contents
extract_dir = '/content/drive/MyDrive/youtube_analysis/youtube_data_mark'


# Create the extraction directory if it doesn't exist
os.makedirs(extract_dir, exist_ok=True)

print(f"Attempting to unzip: {zip_file_path}")

# Unzip the archive
try:
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)
    print(f"Archive extracted to: {extract_dir}")
except FileNotFoundError:
    print(f"Error: The file {zip_file_path} was not found. Please check the path.")
except Exception as e:
    print(f"An error occurred during extraction: {e}")

import pandas as pd

# Path to the youtube_data.csv file
csv_file_path = '/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_data.csv'

# Read the CSV file into a pandas DataFrame
try:
    df = pd.read_csv(csv_file_path)
    print("CSV file loaded successfully.")
    print("Columns in the CSV file:")
    print(df.columns)

    # Assuming there is a column named 'video_id' or similar containing the YouTube video IDs
    # You might need to adjust the column name based on your CSV file
    if 'video_id' in df.columns:
        video_ids = df['video_id'].tolist()
        print(f"\nExtracted {len(video_ids)} video IDs.")
        # Display the first few video IDs
        print("First 5 video IDs:")
        print(video_ids[:5])

        # Now you can use this 'video_ids' list with your yt-dlp code
        # For example, you could update the 'video_ids' list in the cell above
        # video_ids = video_ids # Uncomment and run this line after executing this cell

    elif 'url' in df.columns:
        # If there is a 'url' column, you might need to extract the video ID from the URL
        print("\nFound a 'url' column. Attempting to extract video IDs from URLs.")
        def extract_video_id(url):
            if 'youtube.com/watch?v=' in url:
                return url.split('v=')[-1].split('&')[0]
            elif 'youtu.be/' in url:
                return url.split('/')[-1].split('?')[0]
            return None

        df['extracted_video_id'] = df['url'].apply(extract_video_id)
        video_ids = df['extracted_video_id'].dropna().tolist()
        print(f"\nExtracted {len(video_ids)} video IDs from URLs.")
        # Display the first few video IDs
        print("First 5 video IDs:")
        print(video_ids[:5])

        # Now you can use this 'video_ids' list with your yt-dlp code
        # For example, you could update the 'video_ids' list in the cell above
        # video_ids = video_ids # Uncomment and run this line after executing this cell

    else:
        print("\nCould not find a column containing video IDs or URLs ('video_id' or 'url').")
        print("Please check the column names in your CSV file.")

except FileNotFoundError:
    print(f"Error: The file {csv_file_path} was not found.")
except Exception as e:
    print(f"An error occurred while reading the CSV file: {e}")

# Display the entire DataFrame as a table
if 'df' in locals():
    display(df)
else:
    print("DataFrame 'df' is not defined. Please ensure the CSV file was loaded successfully in the previous cell.")

"""## Step 2: yt-dlp (Download the audio file)"""

!ls -l "/content/drive/MyDrive/youtube_analysis/youtube_audio_files_mark"

import pandas as pd
import os
import subprocess
import shutil  # <-- added for zip creation

# Path to the youtube_data.csv file
csv_file_path = '/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_data.csv'

# Path to save the downloaded audio files
output_dir = '/content/drive/MyDrive/youtube_analysis/youtube_audio_files_mark'

# Create the output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
# IMPORTANT: Path to your cookies file
cookies_file_path = '/content/drive/MyDrive/youtube_analysis/cookies.txt'
# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<

# Read CSV
try:
    df = pd.read_csv(csv_file_path)
    print("CSV file loaded successfully.")
except FileNotFoundError:
    print(f"Error: The file {csv_file_path} was not found.")
    df = None
except Exception as e:
    print(f"An error occurred while reading the CSV file: {e}")
    df = None

# Download audio
if df is not None:
    if 'video_id' in df.columns:
        video_ids = df['video_id'].tolist()
        print(f"\nExtracted {len(video_ids)} video IDs.")

        if 'video_ids_for_download' in locals() and video_ids_for_download:
            print(f"Using the subset of video IDs ({len(video_ids_for_download)}) for download.")
            videos_to_process = video_ids_for_download
        else:
            print("Using the full list of video IDs for download as no subset was defined.")
            videos_to_process = video_ids

        # Adjust range as needed
        start_index = 0
        end_index = 2000
        selected_video_ids = video_ids[start_index:end_index]
        print(f"Selected {len(selected_video_ids)} video IDs from index {start_index} to {end_index}.")

        if selected_video_ids:
            download_count = 0
            for video_id in selected_video_ids:
                download_count += 1
                expected_file = os.path.join(output_dir, f"{video_id}.mp3")

                if os.path.exists(expected_file):
                    print(f"\nSkipping: {video_id} ({download_count}/{len(selected_video_ids)}) - Already exists.")
                    continue

                print(f"\nDownloading audio for {video_id} ({download_count}/{len(selected_video_ids)})...")
                command = f'yt-dlp -x --audio-format mp3 --cookies "{cookies_file_path}" -o "{output_dir}/%(id)s.%(ext)s" -- "{video_id}"'
                try:
                    result = subprocess.run(command, shell=True, capture_output=True, text=True)
                    result.check_returncode()
                except subprocess.CalledProcessError as e:
                    print(f"❌ Error downloading {video_id}. Exit code: {e.returncode}")
                    print("stderr:", e.stderr)
                except Exception as e:
                    print(f"❌ Unexpected error for {video_id}: {e}")
        else:
            print("No YouTube IDs found in the selected range.")

    elif 'url' in df.columns:
        print("\nFound 'url' column. Extracting video IDs...")
        def extract_video_id(url):
            if 'youtube.com/watch?v=' in url:
                return url.split('v=')[-1].split('&')[0]
            elif 'youtu.be/' in url:
                return url.split('/')[-1].split('?')[0]
            return None

        df['extracted_video_id'] = df['url'].apply(extract_video_id)
        video_ids = df['extracted_video_id'].dropna().tolist()
        print(f"Extracted {len(video_ids)} video IDs from URLs.")

        start_index = 0
        end_index = 2000
        selected_video_ids = video_ids[start_index:end_index]
        print(f"Selected {len(selected_video_ids)} video IDs from index {start_index} to {end_index}.")

        if selected_video_ids:
            download_count = 0
            for video_id in selected_video_ids:
                download_count += 1
                expected_file = os.path.join(output_dir, f"{video_id}.mp3")
                if os.path.exists(expected_file):
                    print(f"\nSkipping {video_id} ({download_count}/{len(selected_video_ids)}) - Already exists.")
                    continue

                print(f"\nDownloading {video_id} ({download_count}/{len(selected_video_ids)})...")
                command = f'yt-dlp -x --audio-format mp3 --cookies "{cookies_file_path}" -o "{output_dir}/%(id)s.%(ext)s" -- "{video_id}"'
                try:
                    result = subprocess.run(command, shell=True, capture_output=True, text=True)
                    result.check_returncode()
                except subprocess.CalledProcessError as e:
                    print(f"❌ Error downloading {video_id}. Exit code: {e.returncode}")
                    print("stderr:", e.stderr)
                except Exception as e:
                    print(f"❌ Unexpected error for {video_id}: {e}")
        else:
            print("No valid YouTube URLs found.")

    else:
        print("No 'video_id' or 'url' column found in CSV.")

import os
import zipfile

# === CONFIG ===
source_folder = "/content/drive/MyDrive/youtube_analysis/youtube_audio_files_mark"
zip_output_path = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_audio_files_mark_2000.zip"

# === ZIP FUNCTION ===
def zip_folder(folder_path, zip_path):
    """Compress the given folder into a ZIP file."""
    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
        for root, _, files in os.walk(folder_path):
            for file in files:
                file_path = os.path.join(root, file)
                arcname = os.path.relpath(file_path, os.path.dirname(folder_path))
                zipf.write(file_path, arcname)
    print(f"📦 Folder successfully zipped to: {zip_path}")

# === RUN ===
if os.path.exists(source_folder):
    print(f"Zipping folder: {source_folder}")
    zip_folder(source_folder, zip_output_path)
else:
    print(f"❌ Source folder not found: {source_folder}")

"""## Step 3: Whisper Model (Transcription)"""

# STEP 3: Transcribe all audio files using Whisper and save to CSV

!pip install -q transformers librosa soundfile torch

import os
import pandas as pd
import torch
import librosa
from transformers import pipeline
from tqdm import tqdm
import zipfile

# ==========================================
# PATHS
# ==========================================
zip_path = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_audio_files_mark_2000.zip"
extract_dir = "/content/youtube_audio_temp"
folder_exteract = "/content/youtube_audio_temp/youtube_audio_files_mark"
csv_input_path = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_data.csv"
csv_output_path = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_k5.csv"

# ==========================================
# UNZIP AUDIO FILES
# ==========================================
print("🔹 Unzipping MP3 files...")
os.makedirs(extract_dir, exist_ok=True)
try:
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extract_dir)
    print(f"✅ Unzipped into: {extract_dir}")
except FileNotFoundError:
    print(f"Error: The zip file was not found at {zip_path}. Please check the path.")
    # Exit or handle the error appropriately if the zip file is essential
except Exception as e:
    print(f"An error occurred during unzipping: {e}")
    # Exit or handle the error appropriately
# ==========================================
# LOAD CSV (original YouTube data)
# ==========================================
try:
    df = pd.read_csv(csv_input_path)
    print(f"✅ Loaded CSV with {len(df)} rows")
except FileNotFoundError:
    print(f"Error: The CSV input file was not found at {csv_input_path}. Please check the path.")
    df = None # Set df to None to prevent further processing
except Exception as e:
    print(f"An error occurred while loading the CSV file: {e}")
    df = None # Set df to None to prevent further processing


# ==========================================
# LOAD WHISPER MODEL
# ==========================================
asr = None # Initialize asr to None
if df is not None: # Only proceed if df was loaded successfully
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"🔹 Using device: {device}")

    try:
        asr = pipeline(
            "automatic-speech-recognition",
            model="openai/whisper-tiny",
            device=0 if device == "cuda" else -1,
            return_timestamps=True # Added to get timestamps
        )
        print("✅ ASR pipeline loaded successfully.")
    except Exception as e:
        print(f"Error loading ASR pipeline: {e}")
        asr = None # Set asr to None if loading fails

# ==========================================
# RUN TRANSCRIPTION AND SAVE INCREMENTALLY
# ==========================================
if df is not None and asr is not None: # Only proceed if df and asr are loaded
    audio_folder = folder_exteract

    # Define the range of rows to process
    start_index = 671  # Change this to the desired starting row index
    end_index = 700  # Change this to the desired ending row index (exclusive)

    # Use the defined range
    df_to_process = df.iloc[start_index:end_index].copy()


    # Check if output CSV exists and load it to resume
    if os.path.exists(csv_output_path):
        try:
            df_output = pd.read_csv(csv_output_path)
            print(f"Resuming: Loaded existing output CSV with {len(df_output)} rows.")
            # Ensure 'transcript' column exists for merging/updating
            if 'transcript' not in df_output.columns:
                 df_output['transcript'] = ''
            # Merge to align based on video_id and keep existing transcripts
            df_to_process = df_to_process.merge(df_output[['video_id', 'transcript']], on='video_id', how='left', suffixes=('', '_existing'))
            df_to_process['transcript'] = df_to_process['transcript_existing'].fillna(df_to_process['transcript'].fillna(''))
            df_to_process = df_to_process.drop(columns=['transcript_existing'])

        except Exception as e:
            print(f"Error loading existing output CSV, starting fresh: {e}")
            df_output = df_to_process.copy()
            df_output['transcript'] = '' # Add transcript column if starting fresh
    else:
        df_output = df_to_process.copy()
        df_output['transcript'] = '' # Add transcript column if starting fresh


    for idx, row in tqdm(df_to_process.iterrows(), total=len(df_to_process)):
        video_id = str(row.get("video_id", ""))
        audio_path = os.path.join(audio_folder, f"{video_id}.mp3")

        # Check if transcription already exists for this video_id
        existing_transcript = df_output.loc[df_output['video_id'] == video_id, 'transcript'].iloc[0] if video_id in df_output['video_id'].values else None
        if existing_transcript and str(existing_transcript).strip() != "":
            print(f"\nSkipping {video_id}: Transcription already exists.")
            continue # Skip to the next video if already transcribed


        if not os.path.exists(audio_path):
            print(f"\nSkipping {video_id}: Audio file not found at {audio_path}")
            # Update df_output with empty transcript for missing file
            if video_id in df_output['video_id'].values:
                df_output.loc[df_output['video_id'] == video_id, 'transcript'] = "Error: Audio file not found"
            else:
                # This case should ideally not happen if df_to_process is based on df_output
                print(f"Warning: video_id {video_id} not found in df_output during missing audio handling.")

            # Save progress after skipping missing audio
            try:
                df_output.to_csv(csv_output_path, index=False)
            except Exception as e:
                print(f"Error saving progress after missing audio for {video_id}: {e}")

            continue


        try:
            result = asr(audio_path)
            transcribed_text = result.get("text", "").strip()

            # Update the corresponding row in df_output with the new transcript
            if video_id in df_output['video_id'].values:
                 df_output.loc[df_output['video_id'] == video_id, 'transcript'] = transcribed_text
            else:
                 # This case should ideally not happen if df_to_process is based on df_output
                 print(f"Warning: video_id {video_id} not found in df_output after transcription.")


            # Save df_output to CSV immediately after successful transcription
            try:
                df_output.to_csv(csv_output_path, index=False)
                # print(f"Saved progress for {video_id}") # Optional: print confirmation for each save
            except Exception as e:
                print(f"Error saving progress to CSV for {video_id}: {e}")


        except Exception as e:
            print(f"⚠️ Error transcribing {video_id}: {e}")
            # Update df_output with an error message for failed transcription
            if video_id in df_output['video_id'].values:
                df_output.loc[df_output['video_id'] == video_id, 'transcript'] = f"Error: {e}"
            else:
                 # This case should ideally not happen
                 print(f"Warning: video_id {video_id} not found in df_output after transcription error.")

            # Save progress after transcription error
            try:
                df_output.to_csv(csv_output_path, index=False)
            except Exception as e:
                print(f"Error saving progress after transcription error for {video_id}: {e}")


    print(f"\n✅ Transcription process complete for the selected videos. Final data saved to: {csv_output_path}")

else:
    print("Transcription skipped due to previous errors in loading data or model.")

import pandas as pd

# file paths
file1 = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_12.csv"
file2 = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_f300.csv"

# read both files
df1 = pd.read_csv(file1)
df2 = pd.read_csv(file2)

# Combine them
# Prioritize df2 if a video_id exists in both and has a non-empty transcript
# Keep non-empty transcript if one is empty and the other is not
# If both are empty or both are non-empty, prioritize df2's transcript
merged = pd.concat([df2, df1], ignore_index=True)

# Sort by video_id to group duplicates
merged = merged.sort_values(by='video_id')

# Drop duplicates based on video_id, keeping the first occurrence after sorting.
# Since we concatenated df2 first and sorted, this logic:
# 1. If video_id is only in df2, it will be kept.
# 2. If video_id is only in df1, it will be kept.
# 3. If video_id is in both, the one from df2 comes first after sorting, so it's kept.
# This doesn't perfectly handle the "non-empty transcript" rule if df1 had a non-empty
# transcript and df2 had an empty one for the same video_id.

# A more robust way to handle the non-empty transcript priority:
# Create a new column indicating if transcript is non-empty
merged['has_transcript'] = merged['transcript'].fillna('').str.strip() != ''

# Sort by video_id and then by whether it has a transcript (True comes before False)
# This ensures rows with transcripts for a video_id appear before rows without.
merged = merged.sort_values(by=['video_id', 'has_transcript'], ascending=[True, False])

# Drop duplicates based on video_id, keeping the first row (which now has priority based on transcript)
merged = merged.drop_duplicates(subset=['video_id'], keep='first')

# Drop the temporary 'has_transcript' column
merged = merged.drop(columns=['has_transcript'])

# Optionally, sort again by video_id for final output order (already sorted, but makes it explicit)
merged = merged.sort_values(by='video_id')


# save merged version
merged.to_csv("/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_f700.csv", index=False)

print("✅ Files merged and saved as youtube_transcript_merged.csv with duplicate handling and sorting.")

"""## Step 4: Translation API"""

# ✅ Step 4: Safe Translation with Decomposition (No GPU Overload)
# Uses deep-translator (Google Translate API) and auto-splits long text chunks.

!pip install deep-translator --quiet

from deep_translator import GoogleTranslator
import pandas as pd
import gc, time

# === CONFIG ===
input_csv = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_f700.csv"   # your input file
output_csv = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_en_f700.csv"  # output file name
source_col = "transcript"      # column with original text
target_col = "transcript_en"   # column for translated text
source_lang = "auto"           # auto-detect
target_lang = "en"             # translate to English
max_chunk_len = 4800           # safe max to avoid 5000-char API limit

# === SPLIT FUNCTION ===
def split_text(text, max_length=max_chunk_len):
    """Split long text into chunks under max_length safely."""
    if not isinstance(text, str) or not text.strip():
        return []
    sentences = text.replace('\n', ' ').split('. ')
    chunks, current = [], ""
    for s in sentences:
        if len(current) + len(s) + 2 > max_length:
            chunks.append(current.strip())
            current = s + ". "
        else:
            current += s + ". "
    if current:
        chunks.append(current.strip())
    return chunks

# === TRANSLATION FUNCTION ===
def safe_translate(text):
    """Translate long text safely and retry on failure."""
    if not isinstance(text, str) or not text.strip():
        return ""
    translated_chunks = []
    for chunk in split_text(text):
        try:
            translated = GoogleTranslator(source=source_lang, target=target_lang).translate(chunk)
            translated_chunks.append(translated)
            time.sleep(0.5)  # avoid rate limit
        except Exception as e:
            print(f"⚠️ Error translating chunk: {e}")
            translated_chunks.append("[TRANSLATION ERROR]")
            time.sleep(2)
    return " ".join(translated_chunks)

# === MAIN PROCESS ===
df = pd.read_csv(input_csv)
df[target_col] = ""

for i, row in df.iterrows():
    try:
        df.at[i, target_col] = safe_translate(row[source_col])
        if i % 10 == 0:
            print(f"✅ Translated row {i}/{len(df)}")
            df.to_csv(output_csv, index=False)
            gc.collect()
    except Exception as e:
        print(f"❌ Error at row {i}: {e}")
        df.at[i, target_col] = "[ERROR]"
        continue

# Final save
df.to_csv(output_csv, index=False)
print("🎉 Translation complete and saved to", output_csv)

import pandas as pd

# === CONFIG ===
input_csv_path = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_en_f700.csv"  # your input file
output_csv_path = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_for_trained.csv" # output file name

# Define the range of rows to cut (exclusive of end_index)
start_index = 0  # Start from the first row (index 0)
end_index = 672 # Cut up to, but not including, row 100

# === PROCESS ===
try:
    # Read the input CSV file
    df = pd.read_csv(input_csv_path)
    print(f"✅ Loaded CSV with {len(df)} rows from {input_csv_path}")

    # Select the specified range of rows
    df_subset = df.iloc[start_index:end_index].copy()
    print(f"✅ Selected {len(df_subset)} rows from index {start_index} to {end_index-1}")

    # Save the subset to a new CSV file
    df_subset.to_csv(output_csv_path, index=False)
    print(f"✅ Subset saved to: {output_csv_path}")

except FileNotFoundError:
    print(f"❌ Error: The input file was not found at {input_csv_path}. Please check the path.")
except Exception as e:
    print(f"❌ An error occurred: {e}")

"""## Step 5: Bag-of-Word (Categorize the video)"""

# ===== STEP 5: Bag-of-Words classifier using English transcripts =====

!pip install -q scikit-learn pandas nltk

import pandas as pd
import re
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# ---------------- Paths ----------------
csv_input_path = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_en.csv"
csv_output_path = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_predicted.csv"

# ---------------- Load CSV ----------------
df = pd.read_csv(csv_input_path)
print(f"✅ Loaded CSV with {len(df)} rows")

# ---------------- Use English transcript ----------------
if "transcript_en" not in df.columns:
    raise ValueError("Column 'transcript_en' not found. Make sure Step 4 has run successfully.")

# Fill missing transcripts with empty string
df["text"] = df["transcript_en"].fillna("")

# Drop rows with empty text (optional, improves classifier training)
df = df[df["text"].str.strip() != ""]
print(f"✅ Using {len(df)} rows with non-empty transcripts")

# ---------------- Check target ----------------
if "category" not in df.columns:
    raise ValueError("Column 'category' not found in CSV. Add your target categories.")

X_text = df["text"]
y = df["category"]

# ---------------- Text cleaning ----------------
def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-z0-9\s]", "", text)
    return text

X_text = X_text.apply(clean_text)

# ---------------- Train-test split ----------------
X_train, X_test, y_train, y_test = train_test_split(
    X_text, y, test_size=0.2, random_state=42 # Removed stratify=y
)

# ---------------- Bag-of-Words Vectorization ----------------
vectorizer = CountVectorizer(stop_words="english", max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# ---------------- Train Classifier ----------------
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train_vec, y_train)

# ---------------- Evaluate ----------------
y_pred = clf.predict(X_test_vec)
print("\n📊 Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ---------------- Save predictions ----------------
df["predicted_category"] = clf.predict(vectorizer.transform(X_text))
df.to_csv(csv_output_path, index=False)
print(f"\n✅ Predictions saved to: {csv_output_path}")

# ---------------- Preview ----------------
print(df[["video_id", "transcript_en", "category", "predicted_category"]].head())

# ===== STEP 5+: Bag-of-Words analysis, visualization, and accuracy comparison =====

!pip install -q scikit-learn pandas nltk matplotlib seaborn

import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# ---------------- Paths ----------------
csv_input_path = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_en.csv"
csv_output_path = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_predicted.csv"

# ---------------- Load CSV ----------------
df = pd.read_csv(csv_input_path)
print(f"✅ Loaded CSV with {len(df)} rows")

# ---------------- Use English transcript ----------------
if "transcript_en" not in df.columns:
    raise ValueError("Column 'transcript_en' not found. Make sure Step 4 has run successfully.")

df["text"] = df["transcript_en"].fillna("")
df = df[df["text"].str.strip() != ""]
print(f"✅ Using {len(df)} rows with non-empty transcripts")

# ---------------- Check target ----------------
if "category" not in df.columns:
    raise ValueError("Column 'category' not found in CSV. Add your target categories.")

X_text = df["text"]
y = df["category"]

# ---------------- Text cleaning ----------------
def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-z0-9\s]", "", text)
    return text

X_text = X_text.apply(clean_text)

# ---------------- Train-test split ----------------
X_train, X_test, y_train, y_test = train_test_split(
    X_text, y, test_size=0.2, random_state=42
)

# ---------------- Bag-of-Words Vectorization ----------------
vectorizer = CountVectorizer(stop_words="english", max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# ---------------- Train Classifier ----------------
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train_vec, y_train)

# ---------------- Evaluate ----------------
y_pred = clf.predict(X_test_vec)
acc = accuracy_score(y_test, y_pred)
print("\n📊 Accuracy:", acc)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ---------------- Save predictions ----------------
df["predicted_category"] = clf.predict(vectorizer.transform(X_text))
df.to_csv(csv_output_path, index=False)
print(f"\n✅ Predictions saved to: {csv_output_path}")

# ---------------- 1️⃣ Word Frequency Plot ----------------
word_counts = X_train_vec.toarray().sum(axis=0)
vocab = vectorizer.get_feature_names_out()
freq_df = pd.DataFrame({"word": vocab, "count": word_counts})
freq_df = freq_df.sort_values("count", ascending=False).head(20)

plt.figure(figsize=(10,5))
sns.barplot(x="count", y="word", data=freq_df, palette="viridis")
plt.title("Top 20 Most Frequent Words (Bag-of-Words)")
plt.xlabel("Frequency")
plt.ylabel("Word")
plt.tight_layout()
plt.show()

# ---------------- 2️⃣ Top Words per Category ----------------
category_top_words = {}
for category in df["category"].unique():
    text_cat = " ".join(df[df["category"] == category]["text"])
    X_cat = vectorizer.transform([text_cat])
    word_freq = X_cat.toarray().flatten()
    top_indices = word_freq.argsort()[::-1][:10]
    top_words = [vocab[i] for i in top_indices]
    category_top_words[category] = top_words

print("\n🧩 Top words per category:")
for cat, words in category_top_words.items():
    print(f"  {cat}: {', '.join(words)}")

# ---------------- 3️⃣ Accuracy comparison with baseline ----------------
# If you want to compare to a previous step (Step 5 origin),
# just manually record the old accuracy and compare here.
previous_accuracy = 0.75  # <-- replace with your Step 5 baseline accuracy
print(f"\n⚖️ Accuracy comparison:")
print(f"Previous model: {previous_accuracy:.3f}")
print(f"Current model : {acc:.3f}")
print(f"Change         : {(acc - previous_accuracy)*100:.2f}%")

# ==========================================
# 🎯 STEP 5: Bag-of-Words Model & Analysis
# ==========================================
# Goals:
# - Use Bag-of-Words to predict categories
# - Display word frequency graphs
# - Show top frequent words per category
# - Show accuracy, translation errors, and blanks

!pip install -q scikit-learn pandas nltk matplotlib seaborn

# ---------------- Imports ----------------
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

# ---------------- Paths ----------------
csv_input_path = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_en.csv"
csv_output_path = "/content/drive/MyDrive/youtube_analysis/youtube_data_mark/youtube_transcript_predicted.csv"

# ---------------- Load CSV ----------------
df = pd.read_csv(csv_input_path)
print(f"✅ Loaded CSV with {len(df)} rows")

# ---------------- Data checks ----------------
if "transcript_en" not in df.columns:
    raise ValueError("❌ Column 'transcript_en' not found. Run translation step first.")
if "category" not in df.columns:
    raise ValueError("❌ Column 'category' not found in CSV. Add or verify categories.")

# ---------------- Detect translation issues ----------------
translation_errors = df["transcript_en"].isna().sum()
blank_transcripts = (df["transcript_en"].fillna("").str.strip() == "").sum()
print(f"\n⚠️ Translation errors (NaN): {translation_errors}")
print(f"⚠️ Blank/empty transcripts : {blank_transcripts}")

# ---------------- Prepare text ----------------
df["text"] = df["transcript_en"].fillna("").astype(str)
df = df[df["text"].str.strip() != ""]
print(f"✅ Using {len(df)} rows with non-empty English transcripts")

# ---------------- Text cleaning ----------------
def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-z0-9\s]", "", text)
    return text

df["clean_text"] = df["text"].apply(clean_text)

# ---------------- Split ----------------
X_train, X_test, y_train, y_test = train_test_split(
    df["clean_text"], df["category"], test_size=0.2, random_state=42
)

# ---------------- Vectorize ----------------
vectorizer = CountVectorizer(stop_words="english", max_features=5000)
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# ---------------- Train ----------------
clf = LogisticRegression(max_iter=1000)
clf.fit(X_train_vec, y_train)

# ---------------- Predict ----------------
y_pred = clf.predict(X_test_vec)
acc = accuracy_score(y_test, y_pred)
print(f"\n📊 Accuracy: {acc:.3f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# ---------------- Save ----------------
df["predicted_category"] = clf.predict(vectorizer.transform(df["clean_text"]))
df.to_csv(csv_output_path, index=False)
print(f"\n✅ Predictions saved to: {csv_output_path}")

# ---------------- 1️⃣ Top 20 Words Overall ----------------
word_counts = X_train_vec.toarray().sum(axis=0)
vocab = vectorizer.get_feature_names_out()
freq_df = pd.DataFrame({"word": vocab, "count": word_counts}).sort_values("count", ascending=False).head(20)

plt.figure(figsize=(10, 5))
sns.barplot(x="count", y="word", data=freq_df, palette="viridis")
plt.title("Top 20 Most Frequent Words (All Categories)")
plt.xlabel("Frequency")
plt.ylabel("Word")
plt.tight_layout()
plt.show()

# ---------------- 2️⃣ Top 10 Words per Category ----------------
category_top_words = {}
for category in df["category"].unique():
    text_cat = " ".join(df[df["category"] == category]["clean_text"])
    X_cat = vectorizer.transform([text_cat])
    word_freq = X_cat.toarray().flatten()
    top_indices = word_freq.argsort()[::-1][:10]
    top_words = [vocab[i] for i in top_indices]
    category_top_words[category] = top_words

print("\n🧩 Top 10 words per category:")
for cat, words in category_top_words.items():
    print(f"  • {cat}: {', '.join(words)}")

# ---------------- 3️⃣ Summary ----------------
print("\n📋 Summary:")
print(f"Total rows               : {len(df)}")
print(f"Translation errors (NaN)  : {translation_errors}")
print(f"Blank transcripts (empty) : {blank_transcripts}")
print(f"Model accuracy            : {acc:.3f}")

print("\n✅ Done! You now have:")
print("- Predicted categories in CSV")
print("- Accuracy & classification report")
print("- Graph of top 20 frequent words")
print("- Top 10 words for each category")
print("- Counts of translation and blank issues")